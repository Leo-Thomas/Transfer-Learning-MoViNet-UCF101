{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GidiisyXwK--"
      },
      "source": [
        "#Install necessary libraries and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nubWhqYdwEXD"
      },
      "outputs": [],
      "source": [
        "!pip install remotezip tqdm opencv-python==4.5.2.52 opencv-python-headless==4.5.2.52 tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QImPsudoK9JI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import remotezip as rz\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from official.vision.configs import video_classification\n",
        "from official.projects.movinet.configs import movinet as movinet_configs\n",
        "from official.projects.movinet.modeling import movinet\n",
        "from official.projects.movinet.modeling import movinet_layers\n",
        "from official.projects.movinet.modeling import movinet_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download, extract, and split UCF101 data set"
      ],
      "metadata": {
        "id": "c9P_jVbPHtzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = 'ucf101'\n",
        "\n",
        "builder = tfds.builder(dataset_name)\n",
        "\n",
        "config = tfds.download.DownloadConfig(verify_ssl=False)\n",
        "builder.download_and_prepare(download_config=config)"
      ],
      "metadata": {
        "id": "11fxJ0K1Hvpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = builder.info.features['label'].num_classes\n",
        "num_examples = {\n",
        "    name: split.num_examples\n",
        "    for name, split in builder.info.splits.items()\n",
        "}\n",
        "\n",
        "print('Number of classes:', num_classes)\n",
        "print('Number of examples for train:', num_examples['train'])\n",
        "print('Number of examples for test:', num_examples['test'])\n",
        "print()\n",
        "\n",
        "builder.info"
      ],
      "metadata": {
        "id": "p1xGkgprH0oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "num_frames = 8\n",
        "frame_stride = 10\n",
        "resolution = 172\n",
        "\n",
        "def format_features(features):\n",
        "  video = features['video']\n",
        "  video = video[:, ::frame_stride]\n",
        "  video = video[:, :num_frames]\n",
        "\n",
        "  video = tf.reshape(video, [-1, video.shape[2], video.shape[3], 3])\n",
        "  video = tf.image.resize(video, (resolution, resolution))\n",
        "  video = tf.reshape(video, (-1, num_frames, resolution, resolution, 3))\n",
        "  video = tf.cast(video, tf.float32) / 255.\n",
        "\n",
        "  label = tf.one_hot(features['label'], num_classes)\n",
        "  return (video, label)\n",
        "\n",
        "train_dataset = builder.as_dataset(\n",
        "    split='train',\n",
        "    batch_size=batch_size,\n",
        "    shuffle_files=True)\n",
        "train_dataset = train_dataset.map(\n",
        "    format_features,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.repeat()\n",
        "train_dataset = train_dataset.prefetch(2)\n",
        "\n",
        "test_dataset = builder.as_dataset(\n",
        "    split='test',\n",
        "    batch_size=batch_size)\n",
        "test_dataset = test_dataset.map(\n",
        "    format_features,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    deterministic=True)\n",
        "test_dataset = test_dataset.prefetch(2)"
      ],
      "metadata": {
        "id": "H_rfOojFH4t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UsxiPs8yA2e"
      },
      "source": [
        "#Download a pre-trained MoViNet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhSCM6cee05F"
      },
      "outputs": [],
      "source": [
        "model_id = 'a1' #---> You can change this for a0 (light), or a2 (robust)\n",
        "resolution = 172\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "backbone = movinet.Movinet(\n",
        "    model_id=model_id,\n",
        "    causal=True,\n",
        "    conv_type='2plus1d',\n",
        "    se_type='2plus3d',\n",
        "    activation='swish',\n",
        "    gating_activation='sigmoid'\n",
        ")\n",
        "backbone.trainable = False\n",
        "\n",
        "# Set num_classes=600 to load the pre-trained weights from the original model\n",
        "model = movinet_model.MovinetClassifier(\n",
        "    backbone, num_classes=600)\n",
        "model.build([1, 1, 1, 1, 3])\n",
        "\n",
        "# Load pre-trained weights, be sure you change de model id\n",
        "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a1_stream.tar.gz -O movinet_a1_stream.tar.gz -q\n",
        "!tar -xvf movinet_a1_stream.tar.gz\n",
        "\n",
        "checkpoint_dir = f'movinet_{model_id}_stream'\n",
        "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "status = checkpoint.restore(checkpoint_path)\n",
        "status.assert_existing_objects_matched()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cfAelbU5Gi3"
      },
      "outputs": [],
      "source": [
        "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes, freeze_backbone=False):\n",
        "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
        "  model = movinet_model.MovinetClassifier(\n",
        "      backbone=backbone,\n",
        "      num_classes=num_classes)\n",
        "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HWSk-u7oPUZ"
      },
      "outputs": [],
      "source": [
        "model = build_classifier(batch_size, num_frames, resolution, backbone, 101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhbX7qdTN8lc"
      },
      "source": [
        "#Finetune on UFC101 data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVqBLrn1tBsd"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "train_steps = num_examples['train'] // batch_size\n",
        "total_train_steps = train_steps * num_epochs\n",
        "test_steps = num_examples['test'] // batch_size\n",
        "\n",
        "\n",
        "loss_obj = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    label_smoothing=0.1)\n",
        "\n",
        "metrics = [\n",
        "    tf.keras.metrics.TopKCategoricalAccuracy(\n",
        "        k=1, name='top_1', dtype=tf.float32),\n",
        "    tf.keras.metrics.TopKCategoricalAccuracy(\n",
        "        k=5, name='top_5', dtype=tf.float32),\n",
        "]\n",
        "\n",
        "initial_learning_rate = 0.01\n",
        "learning_rate = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate, decay_steps=total_train_steps,\n",
        ")\n",
        "optimizer = tf.keras.optimizers.RMSprop(\n",
        "    learning_rate, rho=0.9, momentum=0.9, epsilon=1.0, clipnorm=1.0)\n",
        "\n",
        "model.compile(loss=loss_obj, optimizer=optimizer, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = f\"movinet_{model_id}_base_checkpoint/cptk-1\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1,)"
      ],
      "metadata": {
        "id": "kc9PvbxHuf1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZeiYzI0tqQG"
      },
      "outputs": [],
      "source": [
        "results = model.fit(train_dataset,\n",
        "                    validation_data=test_dataset,\n",
        "                    epochs=num_epochs,\n",
        "                    steps_per_epoch=train_steps,\n",
        "                    validation_steps=test_steps,\n",
        "                    validation_freq=1,\n",
        "                    callbacks=[cp_callback],\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.history"
      ],
      "metadata": {
        "id": "_uckjQbKT0Ez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd8ca6b-ca46-4767-e106-19a22c04f503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [2.121842384338379,\n",
              "  1.2998961210250854,\n",
              "  1.1516916751861572,\n",
              "  1.086182951927185,\n",
              "  1.0599242448806763],\n",
              " 'top_1': [0.688129186630249,\n",
              "  0.9031378030776978,\n",
              "  0.9534053802490234,\n",
              "  0.9732396006584167,\n",
              "  0.9774373173713684],\n",
              " 'top_5': [0.8741610646247864,\n",
              "  0.9909749031066895,\n",
              "  0.9966418147087097,\n",
              "  0.9986357688903809,\n",
              "  0.9989506006240845],\n",
              " 'val_loss': [1.754421591758728,\n",
              "  1.6787440776824951,\n",
              "  1.636000394821167,\n",
              "  1.6157978773117065,\n",
              "  1.6128500699996948],\n",
              " 'val_top_1': [0.742849588394165,\n",
              "  0.7449682354927063,\n",
              "  0.7677436470985413,\n",
              "  0.7688029408454895,\n",
              "  0.7725105881690979],\n",
              " 'val_top_5': [0.9298199415206909,\n",
              "  0.945974588394165,\n",
              "  0.9472987055778503,\n",
              "  0.9510063529014587,\n",
              "  0.9502118825912476]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights=model.get_weights()\n",
        "len(weights)"
      ],
      "metadata": {
        "id": "9gbPIijcaRN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Export model (TFlite)"
      ],
      "metadata": {
        "id": "_-KR4OwG48WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from official.projects.movinet.tools import export_saved_model"
      ],
      "metadata": {
        "id": "7ej4Hu0tLs0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = [1, 1, 172, 172, 3]\n",
        "batch_size, num_frames, image_size, = input_shape[:3]\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "# Create the model\n",
        "input_specs = tf.keras.layers.InputSpec(shape=input_shape)\n",
        "stream_backbone = movinet.Movinet(\n",
        "    model_id='a1',\n",
        "    causal=True,\n",
        "    input_specs=input_specs,\n",
        "    conv_type='2plus1d',\n",
        "    se_type='2plus3d',\n",
        "    activation='swish',\n",
        "    gating_activation='sigmoid',\n",
        "    use_external_states=True)\n",
        "stream_backbone.trainable=False\n",
        "stream_model = movinet_model.MovinetClassifier(\n",
        "    backbone=stream_backbone,\n",
        "    num_classes=101,\n",
        "    output_states=True)\n",
        "stream_model.build([1, 1, 172, 172, 3])"
      ],
      "metadata": {
        "id": "zY5WnC-7SfVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stream_model.set_weights(weights)"
      ],
      "metadata": {
        "id": "MYuSn3A_4-du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stream_model.get_weights()[0]"
      ],
      "metadata": {
        "id": "b0BzXJ5P0R6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_weights()[0]"
      ],
      "metadata": {
        "id": "XovdMG7uA_mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_dir=f\"/my_model/movinet_{model_id}_stream_UCF101\"\n",
        "export_saved_model.export_saved_model(\n",
        "    model=stream_model,\n",
        "    input_shape=input_shape,\n",
        "    export_path=saved_model_dir,\n",
        "    causal=True,\n",
        "    bundle_input_init_states_fn=False)"
      ],
      "metadata": {
        "id": "eAtAGeesLZ_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(f'movinet_{model_id}_stream.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "iSakIFcrM9XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load TFlite model"
      ],
      "metadata": {
        "id": "VOFi2wP60ze2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the interpreter and signature runner\n",
        "interpreter = tf.lite.Interpreter(model_path=f'movinet_{model_id}_stream.tflite')\n",
        "runner = interpreter.get_signature_runner()\n",
        "\n",
        "init_states = {\n",
        "    name: tf.zeros(x['shape'], dtype=x['dtype'])\n",
        "    for name, x in runner.get_input_details().items()\n",
        "}\n",
        "del init_states['image']"
      ],
      "metadata": {
        "id": "w9WIaQ3UUtsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download example gif\n",
        "!wget https://github.com/tensorflow/models/raw/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/jumpingjack.gif -O jumpingjack.gif -q\n",
        "\n",
        "with tf.io.gfile.GFile(\"ufc101_label_map.txt\") as f:\n",
        "  lines = f.readlines()\n",
        "  KINETICS_600_LABELS_LIST = [line.strip() for line in lines]\n",
        "  KINETICS_600_LABELS = tf.constant(KINETICS_600_LABELS_LIST)\n",
        "\n",
        "def load_gif(file_path, image_size=(172, 172)):\n",
        "  \"\"\"Loads a gif file into a TF tensor.\"\"\"\n",
        "  with tf.io.gfile.GFile(file_path, 'rb') as f:\n",
        "    video = tf.io.decode_gif(f.read())\n",
        "  video = tf.image.resize(video, image_size)\n",
        "  video = tf.cast(video, tf.float32) / 255.\n",
        "  return video\n",
        "\n",
        "def get_top_k(probs, k=5, label_map=KINETICS_600_LABELS):\n",
        "  \"\"\"Outputs the top k model labels and probabilities on the given video.\"\"\"\n",
        "  top_predictions = tf.argsort(probs, axis=-1, direction='DESCENDING')[:k]\n",
        "  top_labels = tf.gather(label_map, top_predictions, axis=-1)\n",
        "  top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
        "  top_probs = tf.gather(probs, top_predictions, axis=-1).numpy()\n",
        "  return tuple(zip(top_labels, top_probs))"
      ],
      "metadata": {
        "id": "s5B-2DO8mofx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Insert your video clip here\n",
        "video = load_gif('jumpingjack.gif', image_size=(172, 172))\n",
        "clips = tf.split(video[tf.newaxis], video.shape[0], axis=1)\n",
        "\n",
        "# To run on a video, pass in one frame at a time\n",
        "states = init_states\n",
        "for clip in clips:\n",
        "  # Input shape: [1, 1, 172, 172, 3] ---> 224, 224 for a2 version\n",
        "  outputs = runner(**states, image=clip)\n",
        "  logits = outputs.pop('logits')[0]\n",
        "  states = outputs\n",
        "\n",
        "probs = tf.nn.softmax(logits)\n",
        "top_k = get_top_k(probs)\n",
        "print()\n",
        "for label, prob in top_k:\n",
        "  print(label, prob)"
      ],
      "metadata": {
        "id": "OMdceww7mTi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkLl2zF8G9W0"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqgbzOiKuxxT"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_dataset, return_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hssSdW9XHF_j"
      },
      "outputs": [],
      "source": [
        "def get_actual_predicted_labels(dataset):\n",
        "   actual = [labels for _, labels in dataset.unbatch()]\n",
        "  predicted = model.predict(dataset)\n",
        "\n",
        "  actual = tf.stack(actual, axis=0)\n",
        "  predicted = tf.concat(predicted, axis=0)\n",
        "  predicted = tf.argmax(predicted, axis=1)\n",
        "\n",
        "  return actual, predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TmTue6THGWO"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
        "  cm = tf.math.confusion_matrix(actual, predicted)\n",
        "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
        "  sns.set(rc={'figure.figsize':(12, 12)})\n",
        "  sns.set(font_scale=1.4)\n",
        "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
        "  ax.set_xlabel('Predicted Action')\n",
        "  ax.set_ylabel('Actual Action')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yticks(rotation=0)\n",
        "  ax.xaxis.set_ticklabels(labels)\n",
        "  ax.yaxis.set_ticklabels(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RK1A1C1HH6V"
      },
      "outputs": [],
      "source": [
        "fg = FrameGenerator(subset_paths['train'], num_frames, training = True)\n",
        "label_names = list(fg.class_ids_for_name.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4AFi2e5HKEO"
      },
      "outputs": [],
      "source": [
        "actual, predicted = get_actual_predicted_labels(test_ds)\n",
        "plot_confusion_matrix(actual, predicted, label_names, 'test')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}